\section{Classification Prediction}

Before performing any sort of classification, we first divided the whole data set into five subsets. The largest subset (about 40 percent) was the training set. This set was used for all training of each iteration of our classifiers. The rest of the data was split into four separate sets. These four sets were used for the purposes of cross-validation of results. For predictive purposes, we created an ensemble of 20 random forests using the scikit learn RandomForestClassifier module. Each forest was built using scikit learn's default parameters and contained 100 trees.

If you recall, our data set is highly imbalanced with approximately 91\% of contests filling successfully. If we were to train our forest on such a skewed data set, it would inevitably over-fitâ€”That is to say that it would tend to predict ``Success'' simply because there were many successful contests in the training set. To address this, we start by balancing the training set via under sampling. To under sample, we get the number of ``Successes'' and ``Fails'' in the data set. Whichever occurs less often, we select all training points of that type and sample a set of equal size from the other type. In our case, failures are always less common, so we select all failures then randomly sample a number of successes to match.

We implemented additional parameters to control the minimum acceptable accuracy for each classifier. If a forest is found to be less accurate than the minimum, it is discarded and new versions are generated until the desired number of forests have been collected. This accuracy is measured as the percent of contests correctly labeled by the Random Forest from the training set.