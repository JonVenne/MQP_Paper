\section{Kalman Filters}
Traditionally, the Kalman Filter (KF) is a tool used to analyse a set of linear data points for an unknown dynamic model. Each data point is passed in 1-by-1 to the filter (thus the k$^{th}$ step involves the k$^{th}$ data point) using the following nomenclature:\newline

$x_{k|k}$ [nx1]:    the k$^{th}$ estimated state space given the first k observations \newline
$x_{k+1|k}$ [nx1]:  the k+1$^{st}$ estimated state space given the first k observations\newline
$F_{k}$ [nxn]:  the state transition function\newline
B_k = ?
U_k = ?\newline
$P_{k|k}$ [nxn]:  the error covariance matrix (the confidence in) $x_{k|k}$\newline
Q [nxn]:    the processing noise (confidence in obervations)\newline
$H_{k}$ [1xn]: the observation model at step k\newline
$z_{k}$ [1x1]: the observation at step k\newline
$y_{k}$ [1x1]: the estimate residual at step k\newline
R [1x1]: the measurement noise (confidence in the predictions)\newline
$S_{k}$ [1x1]: the innovation covariance\newline
$K_{k}$ [nx1]: the kalman gain\newline

Assuming a state space with n parameters, each of these is a matrix with dimensions [AxB] meaning it has A rows and B columns. There exists a proper pairing of Q and R for each dataset, however, they are usually not known. They represent artifacts of the Kalman filters' assumptions. Q is the covariance matrix of a multivariate normal distribution with mean 0 for the variability in each of the n parameters in $x_{k} \forall k$. R is the variance of the normal distribution with mean 0 for the variability in $z_{k}$  \forall k.

Given the appropriate values of Q and R, the KF acts as an optimal estimator as it minimizes the Mean Square Error (MSE) of the predicted x. In practice, this can be thought of as nearly equivalent to a recursive weighted least squares (WLS) estimate where Q acts as the forgetting factor for the KF as lambda does in WLS. 

\subsection{Our Application of the Kalman Filter}
Normally, the Kalman Filter is used to smooth out noise while maintaining the general form of the original data. As seen in Figure 3.1, the KF takes the set of noisy observations and is able to reconstruct a good approximation of the true function's behavior.

\begin{figure}[h]
\centering
\includegraphics{body/methodology/q2_kalman.jpg}
\caption{Temperary Kalman Filter Picture}
\end{figure}

 We chose to instead use it to approximate the parameters to a function that best fits the data and from those parameters, forecast what the value should be in the future. Our implementation is as follows:

We will assume we have some set of noisy linear data in the form (time, value) such that we want to find the best linear approximation of the form 

\begin{equation} 
Value = \hat{B_{0}} + \hat{B_{1}}*time  
\end{equation}

that fits this data. We start with an initial state space estimate of $x_{0|0}$, error covariance matrix estimate $P_{0|0}$ and chosen values of Q and R.

\begin{align}
    x_{0|0} &= \begin{bmatrix}
           B_{0} \\
           B_{1}
         \end{bmatrix}
  \end{align}
  
  The first step is prediction. We calculate $x_{k+1|k}$ and $P_{k+1|k}$ at the next iteration based on the current $x_{k|k}$ and $P_{k|k}$.
  
  \begin{subequations}
  \begin{align}
  \hat{x}_{k+1|k} = F_{k} \hat{x}_{k|k}+B_{k}u_{k}   \\
  P_{k+1|k} = F_{k} P_{k|k}F_{k}^{T}+Q
  \end{align}
  \end{subequations}
  
  For our purposes, we've assumed $F_{k}$ is always identity.
  
  \begin{align}
    F_{k} &= \begin{bmatrix}
           1&0 \\
           0&1
         \end{bmatrix}
         \forall k
  \end{align}
  
  Since any matrix multiplied by identity is always the original matrix, this reduces our prediction equations to the form
  
  \begin{subequations}
  \begin{align}
  \hat{x}_{k+1|k} = \hat{x}_{k|k}+B_{k}u_{k}   \\
  P_{k+1|k} = P_{k|k}+Q
  \end{align}
  \end{subequations}
  
  This is a simplification as we know the true function is not constant. This changes the filter from dynamic to a static model approximation. P represents the confidence in (or variability of) each parameter in the current state space with larger values of P meaning less confidence in \hat{x}. From Equ. 3.5b we can see that providing a larger Q causes consistently larger estimates of P. This makes sense as Q is a measure of the variability in each parameter of x, so larger Q's should cause the KF to be less confident in it's predictions.
  
  As a further simplification, we assumed P and Q to be 0 in the off diagonal. This means we assume that the parameters B$_{0}$ and B$_{1}$ exist and change independently where  P$_{B_{0}}$,  P$_{B_{1}}$, Q$_{B_{0}}$, and Q$_{B_{1}}$ can be any positive numbers.
    \[
    P_{k} &= \begin{bmatrix}
           P_{B_{0}}&0 \\
           0&P_{B_{1}}
         \end{bmatrix}
         
    Q &= \begin{bmatrix}
        Q_{B_{0}}&0 \\
        0&Q_{B_{1}}
        \end{bmatrix}
  \]
  The second step is updating. Each iteration of the KF utilizes a single data point, so the k$^{th}$ iteration will use the point (Time$_{k}$, Value$_{k}$). We begin by calculating the residual (or error from the k$^{th}$ known observation) for (Time$_{k}$, Value$_{k}$).
  
  \begin{equation}
  \centering
  y_{k} = z_{k} - H_{k}\hat{x}_{k+1|k}
  \end{equation}
  
  Here H$_{k}$ is
  
  \begin{align}
    H_{k} &= \begin{bmatrix}
           1&Time_{k}
         \end{bmatrix}
  \end{align}
  
  From Equ. 3.2, we can see 
  
  \begin{equation}
  \centering
H_{k}\hat{x}_{k+1|k} = B_{0}*1 + B_{1}*Time_{k}
  \end{equation}
  
  Thus, the y$_{k}$ is simply the difference between the prediction of Value$_{k}$ at Time$_{k}$ and the actual observed Value$_{k}$ at Time$_{k}$.
  
  We then perform the innovation step where we calculate S$_{k}$. S$_{k}$ can be understood as a metric for the confidence in observation z$_{k}$ as it represents the variability of z$_{1}$, \cdots , z$_{k}$. If the values of z tend to vary greatly, S$_{k}$ be large. If the values of z vary by only small amount, S$_{k}$ will also be small. Larger values of R also cause larger values of S$_{k}$ since R is a measure of the variability for all observations.
  
  \begin{equation}
  \centering
s_{k} = R + H_{k}P_{k|k-1}H_{k}^{T}
  \end{equation}
  
  P, H, and s come together to form K$_{k}$, the Kalman gain. Kalman gain can be thought of as a “velocity factor” for the KF controlling the magnitude of adjustment to make to the current x$_{k}$. The formula for the optimal gain, meaning it minimizes the mean square error, is  
  
    \begin{equation}
  \centering
K_{k} = P_{k|k-1}H_{k}^{T}s_{k}^{-1}
  \end{equation}
  
  From this we can see that as P$_{k}$ gets small, so too does K$_{k}$. This is because a large P$_{k}$ implies low confidence (or high variability) in x$_{k}$. Thus, having high confidence in the current state should yield only a small change to the new predicted x$_{k}$. We can also see that as s$_{k}$ gets large K$_{k}$ gets small. This also makes sense since large s$_{k}$ implies low observation confidence (or high observation variability). In that case we'd want a smaller state adjustment for larger prediction errors as we don't trust the  current observation as true (that is we want our state adjustment to be less sensitive to erroneous predictions).
  
  We then improve the current state space estimate using information from the k$^{th}$ iteration thus shifting $\hat{x}_{k|k-1}$ to $\hat{x}_{k|k}$
  
      \begin{subequations}
      \begin{align}
    \Delta\hat{x}_{k} = K_{k}y_{k} \\
    \hat{x}_{k|k} = \hat{x}_{k|k-1} + \Delta\hat{x}_{k}
    \end{align}
  \end{subequations}

From Equ. 3.11a we see that $y_{k}$ controls the sign of the state prediction adjustment. When z$_{k}$ < H$_{k}$X$_{k|k-1}$, y$_{k}$ < 0 making $\Delta\hat{x}_{k}$ negative. We can also see that the magnitude of y$_{k}$ and K$_{k}$ controls the magnitude of $\Delta\hat{x}_{k}$.

We perform the same improvement for P, changing P$_{k|k-1}$ to P$_{k|k}$ by

\begin{equation} 
\centering
P_{k|k} = (I - K_{k}H_{k})P_{k|k-1}(I - K_{k}H_{k})^{T} + K_{k}RK_{k}^{T}
\end{equation}

where I is the 2-by-2 identity matrix. When using the optimal Kalman gain, as we do, this calculation can be reduced to 

\begin{equation} 
\centering
P_{k|k} = (I - K_{k}H_{k})P_{k|k-1}
\end{equation}

Each iteration k will use the previous iterations estimates of $\hat{x}_{k-1|k-1}$ and P$_{k-1|k-1}$ as the new starting guess for $\hat{x}$ and P while maintaining the same Q and R throughout. Once completed for all data points, the final $\hat{x}_{k|k}$ is treated as the model prediction. We can then use those values of B$_{0}$ and B$_{1}$ to forecast what the value will be for some time after the last data point. The nature of the KF allows this data to be processed in any order. Since our data comes in chronilogically and is monotonically increasing over the entire domain, the use of the forgetting Q allows us to essentially weight the later data points more heavily. This is effectively equivalent to performing a WLS fit

Figure 3.2 shows an example of a line whose parameters were found using the KF with an all zero Q along side the same contest fit with a line by ordinary least squares. Both approaches predict virtually the same line. However, if Q is changed such that Q$_{B_{0}}$ = 0.3  instead of 0 as in figure 3.3, we can see the behavior change significantly.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{body/methodology/KF_Q0.png}
\includegraphics[width=8cm]{body/methodology/LS_pred.png}
\caption{Temp linear KF Q=0/ LS predicitons Lam=1}
\end{figure}

Performing the fit by KF can be convenient for time series data as each iteration involves processing only one data point at a time. And while least squares is not very computationally intensive, it still requires redoing the entire dataset calculation when updating the parameters.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{body/methodology/KF_Q3.png}
\caption{Temp lin KF Q=0.3}
\end{figure}

\subsection{Extended Kalman Filter}

While the KF is an excellent method for estimating model parameters, it is limited in the same way as LS in that it can only predict for linear models. This will work for estimating on logged data, however, we'd prefer to be able to directly estimate the parameters of the non-linear exponential function M(t) = $B_{0}e^{B_{1}t}$. To do this, we turn to the Extended Kalman Filter (EKF). The EKF works exactly the same as the normal KF except it can work with non-linear model functions. The only difference between the KF and EKF for our purposes is the values of H$_{k}$. For the EKF, H$_{k}$ takes the form

\begin{equation}
\centering
H_{k} &= \begin{bmatrix}
           {\frac{\partial M(t)}{\partial B_{1}}&\cdots&\frac{\partial M(t)}{\partial B_{i}}
         \end{bmatrix}
\end{equation}

where B$_{0}$ \cdots B$_{i}$ are the values of the state space \hat{x}. In our case, we assume M(t) = $B_{0}e^{B_{1}t}$, thus

\begin{equation}
\centering
H_{k} &= \begin{bmatrix}
           e^{B_{1}t}&B_{1}B_{0}e^{B_{1}t}
         \end{bmatrix}
\end{equation}

Other the calculations are exactly the same as for the ordinary Kalman filter. Figure 3.4 shows some exponential model predictions using the extended Kalman Filter. R = 30 for both, but the left one has all 0's for Q while the right one has 10's on the main diagonal of Q. 

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{body/methodology/KF_True0.png}
\includegraphics[width=8cm]{body/methodology/KF_True10.png}
\caption{Temp EKF Q=0/ EKF Q=10}
\end{figure}